{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple1/Projects/SoundCloud/attention.py\n",
      "/Users/apple1/Projects/SoundCloud/new_data\n",
      "/Users/apple1/Projects/SoundCloud/new_data.csv\n",
      "/Users/apple1/Projects/SoundCloud/summarization.ipynb\n",
      "/Users/apple1/Projects/SoundCloud/classification.ipynb\n",
      "/Users/apple1/Projects/SoundCloud/new_data_60000\n",
      "/Users/apple1/Projects/SoundCloud/vcrawling.ipynb\n"
     ]
    }
   ],
   "source": [
    "#로컬 파일 경로 \n",
    "import os \n",
    "path=os.getcwd()\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df_y = pd.read_pickle(\"/Users/apple1/Projects/SoundCloud/new_data_60000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>pretreatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>this is a great cd full of worship favorites!!...</td>\n",
       "      <td>great worship cd</td>\n",
       "      <td>great cd full worship favorite   time great ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>so creative!  love his music - the words, the ...</td>\n",
       "      <td>gotta listen to this!</td>\n",
       "      <td>creative  love music  word  message  favorite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>keith green, gone far to early in his carreer,...</td>\n",
       "      <td>great approach still gets the message out</td>\n",
       "      <td>keith green  gone far early carreer  left u go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>keith green had his special comedy style of ch...</td>\n",
       "      <td>great a must have</td>\n",
       "      <td>keith green special comedy style chirstian mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>keith green / so you wanna go back to egypt......</td>\n",
       "      <td>a great one from keith with a guest appearance...</td>\n",
       "      <td>keith green  wan na go back egypt  : album kei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText  \\\n",
       "0      5.0  this is a great cd full of worship favorites!!...   \n",
       "1      5.0  so creative!  love his music - the words, the ...   \n",
       "2      5.0  keith green, gone far to early in his carreer,...   \n",
       "3      5.0  keith green had his special comedy style of ch...   \n",
       "4      5.0  keith green / so you wanna go back to egypt......   \n",
       "\n",
       "                                             summary  \\\n",
       "0                                   great worship cd   \n",
       "1                              gotta listen to this!   \n",
       "2          great approach still gets the message out   \n",
       "3                                  great a must have   \n",
       "4  a great one from keith with a guest appearance...   \n",
       "\n",
       "                                        pretreatment  \n",
       "0  great cd full worship favorite   time great ke...  \n",
       "1  creative  love music  word  message  favorite ...  \n",
       "2  keith green  gone far early carreer  left u go...  \n",
       "3  keith green special comedy style chirstian mus...  \n",
       "4  keith green  wan na go back egypt  : album kei...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Replacement not allowed with overlapping keys and values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1f6433a2e650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#overall 1,2 -> 0(부정)/ 4,5->1(긍정)으로 변환해야 함.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreplace\u001b[0;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[1;32m   4040\u001b[0m                                               \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m                                               \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4042\u001b[0;31m                                               method=method)\n\u001b[0m\u001b[1;32m   4043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shift'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreplace\u001b[0;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[1;32m   6486\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6487\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6488\u001b[0;31m                         raise ValueError(\"Replacement not allowed with \"\n\u001b[0m\u001b[1;32m   6489\u001b[0m                                          \"overlapping keys and values\")\n\u001b[1;32m   6490\u001b[0m                     \u001b[0mto_rep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Replacement not allowed with overlapping keys and values"
     ]
    }
   ],
   "source": [
    "#overall 1,2 -> 0(부정)/ 4,5->1(긍정)으로 변환해야 함. \n",
    "df2=df_y.replace({'overall': {1.0: 0, 2.0: 0, 4.0: 1, 5.0: 1}}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 부정 긍정 라벨 몇개인지 확인. 밸런스 안맞으면 데이터 잘라서 맞추기  코드 추가하기 \n",
    "import matplotlib.pyplot as plt\n",
    "stars=df2_y['sentiment']\n",
    "print(sorted(stars.unique())) #고유값 나열\n",
    "plt.hist(stars, bins=2, rwidth=0.85)\n",
    "plt.title('Distribution of label')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_y=[]\n",
    "for line in df_y['retreatment']:\n",
    "    texts_y.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts_y),texts_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import reuters\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "src_tokenizer = Tokenizer()\n",
    "src_tokenizer.fit_on_texts(text_y)\n",
    "\n",
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최대 길이 : 991\n",
      "텍스트의 평균 길이 : 26.23775\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summary_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ebc20fe501ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'텍스트의 평균 길이 : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'요약의 최대 길이 : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'요약의 평균 길이 : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_len' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text_len = [len(s.split()) for s in df_y['pretreatment']]\n",
    "\n",
    "#print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "#print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab=1230\n",
    "maxlen2=50\n",
    "t2=Tokenizer(num_words = src_vocab)\n",
    "t2.fit_on_texts(texts_y)\n",
    "sequences2=t2.texts_to_sequences(texts_y)\n",
    "texts_pad=sequence.pad_sequences(sequences2,maxlen=maxlen2,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm 모델1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data, test data 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts_pad, to_categorical(df2_y['sentiment']), test_size = 0.1, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = Sequential()\n",
    "y_model.add(Embedding(4300, 120))\n",
    "y_model.add(LSTM(120))\n",
    "y_model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('you2_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history4 = y_model.fit(x_train, y_train, batch_size=128, epochs=100, callbacks=[es, mc], validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('you2_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_plot(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm 모델 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold=StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "cvscores=[]\n",
    "X=texts_pad\n",
    "Y=df2_y['sentiment']\n",
    "num=0\n",
    "index_right=[]\n",
    "index_wrong=[]\n",
    "\n",
    "\n",
    "        for train, test in kfold.split(X,Y):\n",
    "            i_r=[]\n",
    "            i_w=[]\n",
    "            y_model = Sequential()\n",
    "            y_model.add(Embedding(4300, 100))\n",
    "            #수정\n",
    "            y_model.add(LSTM(100, return_sequences=True, recurrent_dropout=0.4))\n",
    "            y_model.add(LSTM(100, return_sequences=True))\n",
    "            y_model.add(Dropout(0.5))\n",
    "            y_model.add(LSTM(100))\n",
    "            y_model.add(Dense(2, activation='softmax'))\n",
    "            y_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "            history3 = y_model.fit(X[train], to_categorical(Y[train]), batch_size=128, epochs=30, verbose=0)\n",
    "            scores=y_model.evaluate(X[test], to_categorical(Y[test]),verbose=0)\n",
    "            print(\"%s: %.2f%%\" % (y_model.metrics_names[1], scores[1]*100))\n",
    "            cvscores.append(scores[1] * 100)\n",
    "            y_model.save_weights('checkpoints_c/lstm3/my_checkpoint'+str(num))\n",
    "\n",
    "\n",
    "            ynew = y_model.predict_classes(X[test])\n",
    "            file_name='predict_count/lstm3/predict_wrong'+str(num)+'.txt'\n",
    "            f = open(file_name, \"w\")\n",
    "            file_name2='predict_count/lstm3/predict_right'+str(num)+'.txt'\n",
    "            f2 = open(file_name2, \"w\")\n",
    "            for i, real in enumerate(Y[test]):\n",
    "        \n",
    "                line=''\n",
    "                for integer in X[test][i]:\n",
    "                    for word, index in t2.word_index.items():\n",
    "                        if index == integer:\n",
    "                            line+=(' '+word)\n",
    "                    #print(line, y_test[i], ynew[i])\n",
    "                if(real!=ynew[i]):\n",
    "                    f.write(\"index: %s, real:%s, predict:%s\\n%s\\n\" % (str(i),str(real),str(ynew[i]),line))\n",
    "                    i_w.append(i)\n",
    "                else: \n",
    "                    f2.write(\"index: %s, real:%s, predict:%s\\n%s\\n\" % (str(i),str(real),str(ynew[i]),line))\n",
    "                    i_r.append(i)\n",
    "            index_wrong.append(i_w)\n",
    "            index_right.append(i_r)\n",
    "                \n",
    "    \n",
    "            f.close\n",
    "            f2.close\n",
    "            num+=1 \n",
    "        print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
